{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手写字符识别\n",
    "   主要实现两个方面内容，用softmax做一个模型，再用2层的神经网络实现一个模型。  \n",
    "   ** 运行环境为win10 + Cuda8.0 + CuDNN_v5.1 **  \n",
    "   注意CuDNN_v5.1是必须的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先从TensorFlow的例子中加载MNIST数据,图像的尺寸为28*28的灰度图  <br/>\n",
    "这里的数据其实是来自于tensorflow.contrib.learn.python.learn.datasets.mnist中，这个可以看github中的代码知道   <br/>\n",
    "\n",
    "** 简单介绍一下数据集：有训练集合测试集两部分,内容如下：**\n",
    "\n",
    "| 文件         |内容       |\n",
    "|:---------------:|:---------:|\n",
    "|train-images-idx3-ubyte.gz  |训练集图片 - 55000张训练图片, 5000张验证图片  |\n",
    "|train-labels-idx1-ubyte.gz  |训练集图片对应的数字标签 |\n",
    "|t10k-images-idx3-ubyte.gz  |测试集图片 - 10000 张 图片 |\n",
    "|t10k-labels-idx1-ubyte.gz  |测试集图片对应的数字标签  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、Softmax进行字符识别\n",
    "分为如下几个部分：\n",
    "1. 读取数据\n",
    "2. 建立softmax回归模型用几千的样本训练\n",
    "3. 准确度检验\n",
    "\n",
    "由于softmax不考虑像素之间相关性，所以输入的数据直接变成一维的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面读取过数据，接下来开始画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1、开始建立一个图\n",
    "sess = tf.InteractiveSession()#启动一个交互会话\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])#x和y_都用一个占位符表示\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10]))#W和b因为需要改变，所以定义为初始化为0的变量\n",
    "b = tf.Variable(tf.zeros(10))\n",
    "\n",
    "#2、建立预测部分的操作节点\n",
    "y = tf.matmul(x,W) + b  #计算wx+b\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)) #计算softmax交叉熵的均值\n",
    "\n",
    "#3、现在已经得到了损失函数，接下来要做的就是最小化这一损失函数，这里用最常用的梯度下降做\n",
    "# 为了用到前几节说过的内容，这里用学习率随训练下降的方法执行\n",
    "global_step = tf.Variable(0, trainable = False)#建立一个可变数，而且这个变量在计算梯度时候不被影响,其实就是个全局变量\n",
    "start_learning_rate = 0.5#这么写是为了清楚\n",
    "#得到所需的学习率，学习率每100个step进行一次变化，公式为decayed_learning_rate = learning_rate * decay_rate ^(global_step / decay_steps)\n",
    "learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, 10, 0.9, staircase=True)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)#梯度下降最小化交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面把所有的建图过程都弄完了，接下来就是训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#这是因为在交互的Session下可以这样写Op.run()，还可以sess.run(tf.global_variables_initializer())\n",
    "tf.global_variables_initializer().run()#初始化所有变量\n",
    "\n",
    "#iteration = 1000, Batch_Size = 100 \n",
    "for _ in range(1000):\n",
    "    batch = mnist.train.next_batch(100)#每次选出100个数据\n",
    "    train_step.run(feed_dict = {x:batch[0], y_: batch[1]})#给Placeholder填充数据就可以了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练完毕，现在需要做的是评估训练结果，这个其实是个很复杂的事，因为需要调整的地方很多，这里忽略大部分如交叉验证调参等问题\n",
    "可以得到一个准确率约为0.92的结果，但是这个不是很理想"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9189\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1)) #首先比较两个结果的差异\n",
    "#这时的correct_prediction应该类似[True, False, True, True]，然后只要转为float的形式再求加和平均就知道准确率了\n",
    "#这里的cast是用于形式转化\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n",
    "#打印出来就可以了，注意这个时候accuracy也只是一个tensor,而且也只是一个模型的代表，还需要输入数据\n",
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、2层的cnn神经网络\n",
    " 因为上面的结果不是很理想，这里进一步的用卷积神经网络试一试\n",
    " 这里首先要设计一下神经网络的构造：  \n",
    " \n",
    "<font color=#DAA520  size=5>输入-->cov1-->relu-->max_pool-->cov2-->relu-->max_pool-->full_connect-->drop_out-->softmax-->full_connect-->结果</font>  \n",
    "\n",
    "<table><tr><td bgcolor=orange>会用到许多tf.nn的内容</td></tr></table>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "步骤还是和上面一样定义变量\n",
    "不过这个时候注意：\n",
    "    * 尺寸变了，直接输入的是一个图像了\n",
    "    * 不是用0来初始化变量W和b了，而是用截断高斯来定义\n",
    "    * 初始化方法变了不是同一进行初始化，这是为了体现另一种初始化方法，实际也可以和上面一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#首先把要重复用的定义好\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape=shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)#常量转变量,\n",
    "    return tf.Variable(initial)\n",
    "def conv2d(x, f):\n",
    "    return tf.nn.conv2d(x, f, strides=[1,1,1,1], padding='SAME')\n",
    "def max_pool_22(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先构造图\n",
    "这里的输入和上面一样，但是注意输入数据要恢复成一张图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()#启动一个交互会话\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])#x和y_都用一个占位符表示\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "#第一层：\n",
    "#1、设计卷积核1\n",
    "fW1 = weight_variable([5,5,1,32])#[height, weight, in_channel, out_channel]\n",
    "fb1 = bias_variable([32])\n",
    "\n",
    "#2、卷积加池化\n",
    "h1 = tf.nn.relu(conv2d(x_image,fW1)+ fb1)\n",
    "h1_pool = max_pool_22(h1)\n",
    "\n",
    "#第二层\n",
    "fW2 = weight_variable([5,5,32,64])#[height, weight, in_channel, out_channel]\n",
    "fb2 = bias_variable([64])\n",
    "\n",
    "h2 = tf.nn.relu(conv2d(h1_pool,fW2)+ fb2)\n",
    "h2_pool = max_pool_22(h2)\n",
    "\n",
    "#全部变成一维全连接层，这里因为是按照官方走的，所以手动计算了经过第二层后的图片尺寸为7*7\n",
    "#来定义了一个wx+b所需的w和b的尺寸，注意这里的W和b不是卷积所用的了\n",
    "h2_pool_flat = tf.reshape(h2_pool, [-1, 7*7*64])#首先把数据变成行表示\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h2_pool_flat, W_fc1) + b_fc1)\n",
    "\n",
    "#定义dropout,选择性失活,首先指定一个失活的比例\n",
    "prob = tf.placeholder(tf.float32)\n",
    "h_dropout = tf.nn.dropout(h_fc1, prob)\n",
    "\n",
    "#最后一个全连接层，输出10个值，用于softmax\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "y_conv = tf.matmul(h_dropout, W_fc2) + b_fc2\n",
    "\n",
    "#梯度更新，这里采用另一种优化方式AdamOptimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来正式的训练模型  \n",
    "+ 这里记住dropout有prob的超参数,一般都是用0.5\n",
    "+ 每100次迭代显示一次训练准确度的结果\n",
    "+ 这里总共用了2000次迭代，每次用50个数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.08\n",
      "step 100, training accuracy 0.76\n",
      "step 200, training accuracy 0.9\n",
      "step 300, training accuracy 0.9\n",
      "step 400, training accuracy 0.96\n",
      "step 500, training accuracy 0.92\n",
      "step 600, training accuracy 0.96\n",
      "step 700, training accuracy 0.94\n",
      "step 800, training accuracy 0.96\n",
      "step 900, training accuracy 0.94\n",
      "step 1000, training accuracy 0.96\n",
      "step 1100, training accuracy 1\n",
      "step 1200, training accuracy 0.94\n",
      "step 1300, training accuracy 0.98\n",
      "step 1400, training accuracy 0.96\n",
      "step 1500, training accuracy 0.98\n",
      "step 1600, training accuracy 0.94\n",
      "step 1700, training accuracy 0.94\n",
      "step 1800, training accuracy 0.98\n",
      "step 1900, training accuracy 1\n",
      "test accuracy 0.9796\n"
     ]
    }
   ],
   "source": [
    "#初始化\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(2000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict = {x:batch[0],y_:batch[1], prob:1.0}) #这里是计算accuracy用的eval,不是在run一个Operation\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], prob: 0.5})\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, prob: 1.0}) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的测试结果是test accuracy = 0.9796， 和官方的差一点点，因为我为了方便只用了2000次循环，官方教程用了20000次\n",
    "而且我的GPU比较老是GTX745，但是训练速度还是很快的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
