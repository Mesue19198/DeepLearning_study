{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard的用法\n",
    "\n",
    "## 简介\n",
    "　　前面我们说过了利用TensorFlow进行模型搭建的基本方法，这一节主要是在上一节的内容上进一步研究模型的可视化问题。模型可视化主要是用TensorBoard进行的，这里给出应用实例，实例内容来自[Visualizing Learning](https://www.tensorflow.org/get_started/summaries_and_tensorboard),是基于一个神经网络进行的,而不是基于cnn的,而且对一些内容进行了更正。在此基础上还融合了[Embedding Visualization](https://www.tensorflow.org/get_started/embedding_viz) 中的代码。并且会在我的blog中进行更详细的分析。\n",
    "\n",
    "<font color= BlueViolet size=5>程序全部是基于win10的</font>  \n",
    "$\\color{BlueViolet}{程序全部是基于win10的}$\n",
    "\n",
    "**---**\n",
    "\n",
    "### 这里首先有几个会用到的概念：  \n",
    "　　1.**命名空间**：为了能够规范化的显示每个节点的名称，如果设置了命名空间则这个空间下的名字都会有一个相同的起始名，如：在\n",
    "with tf.name_scope('input'):下的所有节点都会自动被命名为input/xxx的格式。同一个命名空间的数据在TensorBoard中会放在一起  \n",
    "　　2.**logdir日志路径**：这里面存放了所有的汇总数据。  \n",
    "　　3.**Embedding**: 可以看成一种映射，PCA和TSNE都是其中的一种方法\n",
    "***\n",
    "### 这里还有几个常用的函数：\n",
    "1. [tf.summary系列](https://www.tensorflow.org/api_guides/python/summary),主要是两类：Summary Ops（汇总Operation）和writing Summaries（写入汇总的方法)\n",
    " 1. 其中Summary Ops主要分两类就是tf.summary.scalar/histogram/image/audio这些生成summary的方法，还有tf.summary.merge/merge_all的汇总全部summary的方法。\n",
    " 2. writing Summaries只要记住有tf.summary.FileWriter这个写入类，其初始化参数是(log_dir，graph=None)即可（还有其他几个参数就不写了因为这里用不到）,他有一个add_summary(summary,global_step=None)可以同时记录迭代数和summary；还有一个add_run_metadata(run_metadata,tag)方法可以记录元数据（tag就是元数据的名称）  \n",
    "2. [tf.RunMetadata](https://www.tensorflow.org/api_docs/python/tf/RunMetadata)  \n",
    "   这个是定义一个元数据，sess.run方法和tf.summary.add_run_metadata都会用到，可以记录占用内存等信息 \n",
    "3. [tf.RunOptions](https://www.tensorflow.org/api_docs/python/tf/RunOptions)  \n",
    "   和上面的类似，是run方法的一个需要的参数，是TensorFlow的运行选项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义所需全局量：常量，如学习率，迭代次数\n",
    "max_step = 1000\n",
    "learning_rate = 0.001\n",
    "drop_out = 0.5\n",
    "log_dir = 'D:/spring1617_assignment2/assignment2/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#为了简便首先把所需要的都用函数封装起来\n",
    "def weight_var(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "def bias_var(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#我们希望可视化变量的内容\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)#把mean放入scalar汇总中\n",
    "        with tf.name_scope('stddev'):\n",
    "            #这里不知道为什么要多加一个命名空间\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))#计算标准差\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))#最大值\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))#最小值\n",
    "        tf.summary.histogram('histogram', var)#分布图，？？？？？？二维的是怎么算的\n",
    "        \n",
    "#定义一个建立一个神经网络，并加入汇总的函数,否则层数很多就会很麻烦\n",
    "def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "    with tf.name_scope(layer_name):\n",
    "        weights = weight_var([input_dim, output_dim])#这里是个二维的\n",
    "        variable_summaries(weights)#进行汇总\n",
    "    with tf.name_scope('biases'):\n",
    "        biases = bias_var([output_dim])\n",
    "        variable_summaries(biases)\n",
    "    with tf.name_scope('Wx_plus_b'):\n",
    "        preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "        tf.summary.histogram('pre_activations', preactivate) #这里是要分布直方图，\n",
    "    activations = act(preactivate, name='activation')\n",
    "    tf.summary.histogram('activations', activations)#可以和前面进行前后对比\n",
    "    return activations\n",
    "\n",
    "#定义一个feed_dict函数方便区分训练和测试\n",
    "def feed_dict(train):\n",
    "    if train:\n",
    "        xs, ys = mnist.train.next_batch(100)\n",
    "        k = drop_out\n",
    "    else:\n",
    "        xs, ys = mnist.test.images, mnist.test.labels\n",
    "        k = 1.0\n",
    "    return {x: xs, y_: ys, prob: k}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面所需的函数已经都封装好了，下面就是开始设计图的过程了,建立会话开始搭建图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'cross_entropy_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "#设计输入\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10], name='y-input')\n",
    "    \n",
    "#这个完全是为了汇总为image后进行可视化    \n",
    "with tf.name_scope('input_reshape'):\n",
    "    image_shaped = tf.reshape(x,[-1, 28, 28, 1])\n",
    "    tf.summary.image('input', image_shaped)\n",
    "\n",
    "#首先搭建一个nn,得到一个500维的输出\n",
    "hidden1 = nn_layer(x, 784, 500, 'layer1')\n",
    "#经过一个drop_out\n",
    "with tf.name_scope('dropout'):\n",
    "    prob = tf.placeholder(tf.float32)\n",
    "    tf.summary.scalar('drop_out_keep_prob', prob)\n",
    "    dropped = tf.nn.dropout(hidden1, prob)\n",
    "    \n",
    "#搭建第二个layer得到输出\n",
    "y = nn_layer(dropped, 500, 10, 'layer2', act=tf.identity)#tf.identity就是一个恒等函数\n",
    "\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
    "    with tf.name_scope('total'):\n",
    "        cross_entropy = tf.reduce_mean(diff)\n",
    "tf.summary.scalar('cross_entropy', cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下去就是优化和训练及评估过程了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_pre = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pre, tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接获取所有的汇总操作，接着在训练之前设置好summary的写入节点存放训练的日志数据，而且**在训练的sess.graph加入训练过程的记录仪，这样在TensorBoard的$\\color{DarkTurquoise}{GRAPH}$窗口就能展示整个计算图的可视化效果** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "test_writer = tf.summary.FileWriter(log_dir + '/test')\n",
    "\n",
    "#初始化训练图\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这里开始对上面的图进行训练\n",
    "* 毎10次进行1次对测试数据的测试，并输出结果把结果和迭代次数写到测试对应的日志数据中\n",
    "* 每100次把TensorFlow的元信息，包括：运算时间和内存占用等记录下来写入训练日志中(可选)\n",
    "tf.RunOpetions定义了一些运行选项是一个proto，这个也是在一本书上看到的，有精力的可以详细看一看这个是怎么写的\n",
    "同理RunMetadata也是proto，这两个在sess.run中都是可选参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at step 0: 0.0845\n",
      "Accuracy at step 10: 0.6867\n",
      "Accuracy at step 20: 0.7982\n",
      "Accuracy at step 30: 0.8485\n",
      "Accuracy at step 40: 0.875\n",
      "Accuracy at step 50: 0.883\n",
      "Accuracy at step 60: 0.8958\n",
      "Accuracy at step 70: 0.9019\n",
      "Accuracy at step 80: 0.9076\n",
      "Accuracy at step 90: 0.9107\n",
      "Adding run metada for 99\n",
      "Accuracy at step 100: 0.9126\n",
      "Accuracy at step 110: 0.9158\n",
      "Accuracy at step 120: 0.9153\n",
      "Accuracy at step 130: 0.9155\n",
      "Accuracy at step 140: 0.9201\n",
      "Accuracy at step 150: 0.924\n",
      "Accuracy at step 160: 0.9245\n",
      "Accuracy at step 170: 0.927\n",
      "Accuracy at step 180: 0.9301\n",
      "Accuracy at step 190: 0.9321\n",
      "Adding run metada for 199\n",
      "Accuracy at step 200: 0.9324\n",
      "Accuracy at step 210: 0.9322\n",
      "Accuracy at step 220: 0.9311\n",
      "Accuracy at step 230: 0.9332\n",
      "Accuracy at step 240: 0.9334\n",
      "Accuracy at step 250: 0.937\n",
      "Accuracy at step 260: 0.9347\n",
      "Accuracy at step 270: 0.9374\n",
      "Accuracy at step 280: 0.9396\n",
      "Accuracy at step 290: 0.9411\n",
      "Adding run metada for 299\n",
      "Accuracy at step 300: 0.9432\n",
      "Accuracy at step 310: 0.9425\n",
      "Accuracy at step 320: 0.9422\n",
      "Accuracy at step 330: 0.9448\n",
      "Accuracy at step 340: 0.9457\n",
      "Accuracy at step 350: 0.9469\n",
      "Accuracy at step 360: 0.9464\n",
      "Accuracy at step 370: 0.9426\n",
      "Accuracy at step 380: 0.9482\n",
      "Accuracy at step 390: 0.946\n",
      "Adding run metada for 399\n",
      "Accuracy at step 400: 0.9482\n",
      "Accuracy at step 410: 0.9474\n",
      "Accuracy at step 420: 0.95\n",
      "Accuracy at step 430: 0.9492\n",
      "Accuracy at step 440: 0.948\n",
      "Accuracy at step 450: 0.9474\n",
      "Accuracy at step 460: 0.9489\n",
      "Accuracy at step 470: 0.9476\n",
      "Accuracy at step 480: 0.9496\n",
      "Accuracy at step 490: 0.9491\n",
      "Adding run metada for 499\n",
      "Accuracy at step 500: 0.9519\n",
      "Accuracy at step 510: 0.9516\n",
      "Accuracy at step 520: 0.9536\n",
      "Accuracy at step 530: 0.9541\n",
      "Accuracy at step 540: 0.955\n",
      "Accuracy at step 550: 0.9522\n",
      "Accuracy at step 560: 0.9535\n",
      "Accuracy at step 570: 0.9538\n",
      "Accuracy at step 580: 0.9539\n",
      "Accuracy at step 590: 0.9565\n",
      "Adding run metada for 599\n",
      "Accuracy at step 600: 0.9556\n",
      "Accuracy at step 610: 0.9566\n",
      "Accuracy at step 620: 0.9562\n",
      "Accuracy at step 630: 0.9562\n",
      "Accuracy at step 640: 0.9562\n",
      "Accuracy at step 650: 0.9554\n",
      "Accuracy at step 660: 0.9566\n",
      "Accuracy at step 670: 0.9558\n",
      "Accuracy at step 680: 0.9555\n",
      "Accuracy at step 690: 0.9592\n",
      "Adding run metada for 699\n",
      "Accuracy at step 700: 0.9577\n",
      "Accuracy at step 710: 0.9572\n",
      "Accuracy at step 720: 0.9571\n",
      "Accuracy at step 730: 0.9574\n",
      "Accuracy at step 740: 0.9591\n",
      "Accuracy at step 750: 0.9594\n",
      "Accuracy at step 760: 0.9597\n",
      "Accuracy at step 770: 0.9604\n",
      "Accuracy at step 780: 0.9579\n",
      "Accuracy at step 790: 0.9587\n",
      "Adding run metada for 799\n",
      "Accuracy at step 800: 0.9601\n",
      "Accuracy at step 810: 0.9595\n",
      "Accuracy at step 820: 0.9603\n",
      "Accuracy at step 830: 0.9614\n",
      "Accuracy at step 840: 0.9619\n",
      "Accuracy at step 850: 0.9613\n",
      "Accuracy at step 860: 0.9596\n",
      "Accuracy at step 870: 0.9635\n",
      "Accuracy at step 880: 0.9618\n",
      "Accuracy at step 890: 0.9622\n",
      "Adding run metada for 899\n",
      "Accuracy at step 900: 0.9597\n",
      "Accuracy at step 910: 0.9615\n",
      "Accuracy at step 920: 0.9621\n",
      "Accuracy at step 930: 0.9618\n",
      "Accuracy at step 940: 0.9638\n",
      "Accuracy at step 950: 0.9643\n",
      "Accuracy at step 960: 0.9643\n",
      "Accuracy at step 970: 0.963\n",
      "Accuracy at step 980: 0.9629\n",
      "Accuracy at step 990: 0.9642\n",
      "Adding run metada for 999\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()#这个用于创建模型的保存器，只有有保存器才能够进行Embedding\n",
    "for i in range(max_step):\n",
    "    if i%10 == 0:\n",
    "        summary, acc = sess.run([merged, accuracy], feed_dict = feed_dict(False))\n",
    "        test_writer.add_summary(summary, i)\n",
    "        print('Accuracy at step %s: %s' % (i, acc))\n",
    "    elif i%100 == 99:\n",
    "        run_options = tf.RunOptions(trace_level = tf.RunOptions.FULL_TRACE)\n",
    "        run_metadata = tf.RunMetadata()#产生元数据\n",
    "        summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True),options = run_options, run_metadata=run_metadata)\n",
    "        #开始写入结果\n",
    "        train_writer.add_summary(summary, i) \n",
    "        train_writer.add_run_metadata(run_metadata, 'step%03d'%i)\n",
    "        saver.save(sess,log_dir+'/model.ckpt')\n",
    "        print('Adding run metada for',i)\n",
    "    else:\n",
    "        summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "        train_writer.add_summary(summary, i)\n",
    "#关闭writer\n",
    "train_writer.close()\n",
    "test_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "此时数据已经写入了logdir中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
