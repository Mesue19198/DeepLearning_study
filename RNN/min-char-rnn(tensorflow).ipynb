{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最简单的RNN使用\n",
    "序列结构有很多种类别：  \n",
    "* one-to-one，如图像分类\n",
    "* one-to-many，如图像转文字\n",
    "* many-to-one，如语义理解判断语句性质\n",
    "* many-to-many，这个有同步和异步两种\n",
    "之所以能够RNN进行序列的转换，在于中间的recurrent transformation（循环传输）只要是定长的，那么上下的输入输出就可以是不定长的。\n",
    "\n",
    "\n",
    "这里的程序是Character-level也就是one-to-one的  \n",
    "主要分为：变量定义、输入编码、输出解码、构造一个rnn、构造整体模型（softmax）、训练、测试几个部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "# 定义常用变量\n",
    "HIDDEN_SIZE =200#这个是h的尺寸，即W_xh中h的维度\n",
    "NUM_STEPS = 50 #每个batch中序列的长度\n",
    "BATCH_SIZE = 64 #batch的数量\n",
    "LR = 0.003\n",
    "\n",
    "#这里的可视化我指的是测试过程生成序列\n",
    "TEMPRATURE = 0.7#同样是可视化时候需要的，进行输出采样\n",
    "SKIP_STEP = 40 #训练多少次进行一次可视化\n",
    "LEN_GENERATED = 300#可视化时生成的序列长\n",
    "log_dir = 'D:/seq2seq/'\n",
    "DATA_PATH = 'arvix_abstracts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vocab_encode(text, vocab):\n",
    "    #最简单的编码，返回一个list包含所有的text中内容所包含的转换后数据\n",
    "    return [vocab.index(x)+1 for x in text if x in vocab]\n",
    "def vocab_decode(array, vocab):\n",
    "    # array为数字列表\n",
    "    # 返回类型为string\n",
    "    return ''.join([vocab[i-1] for i in array])\n",
    "# 首先看一下数据的读取形式\n",
    "def read_data(filename, vocab, windows=NUM_STEPS, overlap=NUM_STEPS):\n",
    "    #按行读取数据\n",
    "    for text in open(filename):\n",
    "        text = vocab_encode(text, vocab) #把text中的数据编码为数字\n",
    "        for start in range(0, len(text)-windows, overlap):\n",
    "            #把数据分成window长的块\n",
    "            chunk = text[start: start+windows]\n",
    "            chunk += [0]*(windows-len(chunk))#这是为了把不够数量的部分用0来pad\n",
    "            yield chunk\n",
    "#数据进行batch\n",
    "def read_batch(stream, batch_size=BATCH_SIZE):\n",
    "    batch = []\n",
    "    for element in stream:\n",
    "        batch.append(element)\n",
    "        #如果数量batch_size足够就返回\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    #如果最后达不到batch_size的数量也返回即可\n",
    "    yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里用到了几个关键的函数\n",
    "[tf.nn.rnn_cell.GRUCell]()  \n",
    "[tf.nn.dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)\n",
    "\n",
    "tf.nn.dynamic_rnn的参数：\n",
    "* out:返回值out维度和seq相同表示的是seq经过rnn的输出值\n",
    "* out_state：这个就是计算过程中的h(t-1),保存着上一个输出状态，在初始条件下h(0)为None\n",
    "\n",
    "这里需要提到一个问题，对于不定长的问题我们用了补零的方式进行，但是这样会影响计算的loss的结果，有两种解决方式：\n",
    "1. 用mask记录哪些是真实值哪些是补零值，然后用模型计算序列得到的结果，但是在计算损失的时候只用计算真实元素的部分\n",
    "```\n",
    "full_loss = tf.nn.softmax_cross_entropy_with_logits(preds, labels)\n",
    "loss = tf.reduce_mean(tf.boolean_mask(full_loss, mask))\n",
    "```\n",
    "2. 让模型知道真实序列的长度，然后只计算真实序列的结果\n",
    "```\n",
    "cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "length = tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)#计算长度\n",
    "output, out_state = tf.nn.dynamic_rnn(cell, seq, length, initial_state)#让rnn知道长度\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def creat_rnn(seq, hidden_size=HIDDEN_SIZE):\n",
    "    '''\n",
    "        这里的seq为三维[batch_num,NUM_STEPS,len(vocab)]\n",
    "    '''\n",
    "    cell = tf.contrib.rnn.GRUCell(hidden_size) #生成一层的含有hidden_size数量的GRU\n",
    "    #cell.zero_state返回一个batch_size*数量的tensor\n",
    "    #这里注意为什么in_state可以定义为placeholder，在哪里用到了\n",
    "    #不同的batch有不一样的状态，所以共有tf.shape(seq)[0]个不同的in_state\n",
    "    #cell.zero_state(tf.shape(seq)[0],tf.float32)既可以初始化cell\n",
    "    in_state = tf.placeholder_with_default(\n",
    "        cell.zero_state(tf.shape(seq)[0],tf.float32), [None, hidden_size])\n",
    "    \n",
    "    #因为不一定所有的seq中的内容都是num_step长的，所以需要进行pad使所有seq长度一样、\n",
    "    #首先需要计算每个seq的长度，tf.sign让seq中大于0的数变为1(对于本例来说没什么作用)，\n",
    "    #reduce_max求每一个batch中每个数据最大值（对于pad的数据这个值是0）\n",
    "    #tf.reduce_sum得到了每个batch中数据的数量\n",
    "    #生成的length为一个list（tensor）返回每一个batch的长度\n",
    "    \n",
    "    length = tf.reduce_sum(tf.reduce_max(tf.sign(seq),2),1)\n",
    "    \n",
    "    out, out_state = tf.nn.dynamic_rnn(cell=cell,\n",
    "                                       inputs=seq,\n",
    "                                       sequence_length=length, \n",
    "                                       initial_state=in_state)\n",
    "    return out, in_state, out_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_model(seq, temp, vocab, hidden_size= HIDDEN_SIZE):\n",
    "    '''\n",
    "        输入： \n",
    "            vocab: 可能包含的字典（这个程序是按字符构造的，不是按单词）\n",
    "            seq: 读取的数据全集，这里seq还是一个二维Tensor\n",
    "            temp:训练的时候用不到\n",
    "        输出：\n",
    "            loss: softmax损失\n",
    "            sample：采样值，其实不用也可以，只要将\n",
    "            in_state: 输入状态h(t-1)\n",
    "            out_state:输出状态h(t)\n",
    "        \n",
    "    '''\n",
    "    seq = tf.one_hot(seq,len(vocab)) #变one-hot，这时的seq是三维的\n",
    "    output, in_state, out_state = creat_rnn(seq, hidden_size)\n",
    "    #添加一个fully_connect\n",
    "    logits = tf.contrib.layers.fully_connected(output, len(vocab),None)\n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits[:, :-1],labels=seq[:, 1:]))\n",
    "    \n",
    "    # sample the next character from Maxwell-Boltzmann Distribution with temperature temp\n",
    "    # it works equally well without tf.exp\n",
    "    #从每一个batch中采样一个结果\n",
    "    sample = tf.multinomial(tf.exp(logits[:, -1] / temp), 1)[:, 0] \n",
    "    return loss, sample, in_state, out_state    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def online_inference(sess, vocab, seq, sample, temp, in_state, out_state, seed='T'):\n",
    "    \"\"\" Generate sequence one character at a time, based on the previous character\n",
    "        每次生成序列中的一个词， 共LEN_GENERATED次\n",
    "    \"\"\"\n",
    "    sentence = seed\n",
    "    state = None\n",
    "    for _ in range(LEN_GENERATED):\n",
    "        batch = [vocab_encode(sentence[-1], vocab)]\n",
    "        feed = {seq: batch, temp: TEMPRATURE}\n",
    "        # for the first decoder step, the state is None\n",
    "        # 开始的时候in_state是None\n",
    "        if state is not None:\n",
    "            feed.update({in_state: state})\n",
    "        #index是输出字符，state是新的状态值\n",
    "        index, state = sess.run([sample, out_state], feed)\n",
    "        sentence += vocab_decode(index, vocab)\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用上面的函数构建图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = (\n",
    "        \" $%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "        \"\\\\^_abcdefghijklmnopqrstuvwxyz{|}\")\n",
    "#这时的seq还是二维的\n",
    "seq = tf.placeholder(tf.int32,[None,None])\n",
    "temp = tf.placeholder(tf.float32)\n",
    "loss, sample, in_state, out_state = create_model(seq, temp, vocab)\n",
    "global_step = tf.Variable(0,dtype=tf.int32, trainable=False, name='global_step')\n",
    "optimizer = tf.train.AdamOptimizer(LR).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在开始进行训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "start = time.time()\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('gist',sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname(log_dir+ 'checkpoint'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    iteration = global_step.eval()\n",
    "    for batch in read_batch(read_data(DATA_PATH, vocab)):\n",
    "        batch_loss,_ = sess.run([loss, optimizer], {seq:batch})\n",
    "        #下面是为了可视化,每SKIP_STEP对结果进行一次输出\n",
    "        if (iteration+1)%SKIP_STEP == 0:\n",
    "            print('Iter {}. \\n    Loss {}. Time {}'.format(iteration, batch_loss, time.time() - start))\n",
    "            online_inference(sess, vocab, seq, sample, temp, in_state, out_state)\n",
    "            start = time.time()\n",
    "            saver.save(sess, log_dir+'checkpoints', iteration)\n",
    "        iteration+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
